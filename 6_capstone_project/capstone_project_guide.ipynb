{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration Database\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "In the final program of this course we will attempt to set up a data-warehouse in Redshift where analysts can run queries against a dataset provided by Udacity. The goal of this dataset will be to provide a basis for analysis for immigration data of the US. The dataset contains 3 different sources, namely 1. Immigration data from I94 filings from the United States government from 2016, 2. US City demographic data and 3. airport data containing basic airport and city data. We will first perform ETL that processes the data in Spark where it is then stored in s3. From there, we schedule ingestion from S3 into Redshift using Airflow.\n",
    "\n",
    "The idea of automation on the latter part is as follows; given infrequent releases of these data sources (in the real world) we would have a notebook (like this one) ready to run whenever a new batch of data is available, this could even be from different sources or formats over time as especially government datasets are notoriously tricky to handle in terms of changing schemas or column categories. After verifying the ETL process in a notebook like this the data would be put in S3. Since we will require a fixed format to be present in S3 we can then safely the ingestion part and we have an automatic process that picks up this data and updates the tables in Redshift.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import re\n",
    "import os\n",
    "from pyspark.sql.functions import count, col, udf, year, month, avg, round, dayofweek, weekofyear, isnull, when, isnan\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fb3d917d32fe:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7feba9a11240>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sessions\n",
    "conf = SparkConf()\n",
    "conf.set(\n",
    "    \"spark.jars.packages\",\n",
    "    \"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.0\")\n",
    "\n",
    "\"\"\"Create a apache spark session.\"\"\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "- Goal: providing a data source for analysts to analyze migration patterns using different input sources like the ones listed below.\n",
    "- Tools: Spark for preprocessing/sanity checks and dumping the data in AWS S3, then Airflow/Python to move the data from S3 to Redshift.\n",
    "- Considerations: it would be cleaner to put everything in 1 pipeline (i.e. start from local files -> ETL -> S3 -> Redshift) but this would make the project much more complex while adding little to my knowledge. In practice I'm seeing that in the companies I have worked these pipelines are often cut in pieces (i.e. different teams own the local files -> S3 part, then another owns the S3 to redshift), so splitting the project like this makes sense in my opinion.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "- I94 Immigration Data: \n",
    "    Source: The U.S. National Tourism and Trade Office and contains various statistics on international visitor arrival in USA. The dataset contains data from 2016. \n",
    "    [https://travel.trade.gov/research/reports/i94/historical/2016.html]\n",
    "- U.S. City Demographic Data: \n",
    "    Source: OpenSoft and contains information about the demographics of all US cities such as average age, sex etc. \n",
    "    [https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Immigration data\n",
    "\n",
    "# use the glob below to obtain all files, here we will be working with one file only\n",
    "# immigration_files = glob.glob(\"../../data/18-83510-I94-Data-2016/*.sas7bdat\")\n",
    "immigration_files = \"../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\"\n",
    "immigration_df = spark.read.format(\"com.github.saurfang.sas.spark\").load(immigration_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Demographics data\n",
    "demographics_file = \"us-cities-demographics.csv\"\n",
    "demographics_df = (spark\n",
    "                       .read\n",
    "                       .format(\"csv\")\n",
    "                        .option(\"delimiter\", \";\")\n",
    "                        .option(\"header\", \"true\")\n",
    "                        .load(demographics_file)\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Immigration data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "For the purpose of our project we are especially interested in data that describes origin and destination, as well as a few basic facts about the immigrants. We potentially want to do side-analysis about airports as well, so we are also including fields related to those. Therefore we will be using the following fields:\n",
    "- cicid -> immigration_id\n",
    "- i94yr -> immigration_year\n",
    "- i94mnth -> immigration_month\n",
    "- i94res -> origin_country\n",
    "- i94port -> arrival_port\n",
    "- arrdate -> arrival_date\n",
    "- i94mode -> port_type\n",
    "- i94addr -> state_of_residence\n",
    "- depdate -> departure_date\n",
    "- i94bir -> immigrant_age\n",
    "- i94visa -> visitor_type\n",
    "- gender -> gender\n",
    "- airline -> airline_code\n",
    "- fltno -> airline_flight_number\n",
    "- visatype -> visa_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Some of these need a bit of work since they are codes, the description file I94_SAS_Labels_Descriptions.SAS provides some conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we get a list of valid ports according to the description\n",
    "# if a port is not in this list or it's from the part of the list\n",
    "# with invalid codes we put it as 'Other'\n",
    "def get_valid_port_list():\n",
    "    i94_sas_label_descriptions = \"I94_SAS_Labels_Descriptions.SAS\"\n",
    "    with open(i94_sas_label_descriptions) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    re_compiled = re.compile(r\"\\'(.*)\\'.*\\'(.*)\\'\")\n",
    "    valid_ports = {}\n",
    "    # line 302 to 892 describes valid ports\n",
    "    for line in lines[302:893]:\n",
    "        results = re_compiled.search(line)\n",
    "        if 'UNKNOWN' or 'UNIDENTIFIED' or 'NOT REPORTED' in results.group(2):\n",
    "            pass\n",
    "        # removing whitespace for readability\n",
    "        valid_ports[results.group(1)] = re.sub(\"(\\s+)\", \"\", results.group(2))\n",
    "    return valid_ports\n",
    "valid_ports = get_valid_port_list()\n",
    "\n",
    "@udf(StringType())\n",
    "def get_valid_port(x):\n",
    "    if x in valid_ports.keys():\n",
    "        return x\n",
    "    else:\n",
    "        return \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we get a list of valid countries according to the description\n",
    "# if a country is not in this list or it's from the part of the list\n",
    "# with invalid codes we put it as 'Other'\n",
    "def get_valid_country_list():\n",
    "    i94_sas_label_descriptions = \"I94_SAS_Labels_Descriptions.SAS\"\n",
    "    with open(i94_sas_label_descriptions) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    re_compiled = re.compile(r\"([0-9]+).*\\'(.*)\\'\")\n",
    "    valid_ports = {}\n",
    "    for line in lines[9:245]:\n",
    "        results = re_compiled.search(line)\n",
    "        valid_ports[int(results.group(1))] = re.sub(\"(\\s+)\", \"\", results.group(2))\n",
    "    return valid_ports\n",
    "valid_countries = get_valid_country_list()\n",
    "\n",
    "@udf(StringType())\n",
    "def get_valid_country(x):\n",
    "    if int(x) in valid_countries.keys():\n",
    "        return valid_countries[x]\n",
    "    else:\n",
    "        return \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parse the 'arrdate' and 'depdate' fields to a valid date\n",
    "@udf(StringType())\n",
    "def convert_to_datetime(x):\n",
    "    if x:\n",
    "        return (datetime(1960, 1, 1).date() + timedelta(x)).isoformat()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parse the 'i94mode' field to a valid port_type\n",
    "@udf(StringType())\n",
    "def convert_mode(x):\n",
    "    mode_dict = {\n",
    "        1.0 : 'Air',\n",
    "        2.0 : 'Sea',\n",
    "        3.0 : 'Land',\n",
    "        9.0 : 'Not reported'\n",
    "    }\n",
    "    if x in mode_dict.keys():\n",
    "        return mode_dict[x]\n",
    "    return 'Not reported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parse the 'i94visa' field to a valid visitor type\n",
    "@udf(StringType())\n",
    "def convert_visa(x):\n",
    "    mode_dict = {\n",
    "        1 : 'Business',\n",
    "        2 : 'Pleasure',\n",
    "        3 : 'Student'\n",
    "    }\n",
    "    if x in mode_dict.keys():\n",
    "        return mode_dict[x]\n",
    "    return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create port from city\n",
    "@udf(StringType())\n",
    "def city_to_port(city):\n",
    "    for key in valid_ports:\n",
    "        if city.lower() in valid_ports[key].lower():\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert fields\n",
    "immigration_df_clean = ((immigration_df\n",
    "        .withColumnRenamed('cicid', 'immigration_id')\n",
    "        .withColumnRenamed('i94yr', 'immigration_year') \n",
    "        .withColumnRenamed('i94mon', 'immigration_month')\n",
    "        .withColumn('origin_country', get_valid_country(col('i94res')))\n",
    "        .withColumn('arrival_port', get_valid_port(col('i94port')))\n",
    "        .withColumn('arrival_date', convert_to_datetime(col('arrdate')))\n",
    "        .withColumn('port_type', convert_mode(col('i94mode')))\n",
    "        .withColumnRenamed('i94addr', 'state_of_residence')\n",
    "        .withColumn('departure_date', convert_to_datetime(col('depdate')))\n",
    "        .withColumnRenamed('i94bir', 'immigrant_age')\n",
    "        .withColumn('visitor_type', convert_visa(col('i94visa')))\n",
    "        .withColumnRenamed('airline', 'airline_code')\n",
    "        .withColumnRenamed('fltno', 'airline_flight_number')\n",
    "        .withColumnRenamed('visatype', 'visa_type')                       \n",
    ").select('immigration_id', 'immigration_year'\n",
    "         , 'immigration_month', 'origin_country'\n",
    "         , 'arrival_port', 'arrival_date', 'port_type'\n",
    "         , 'state_of_residence'\n",
    "         , 'immigrant_age', 'visitor_type', 'airline_code'\n",
    "         , 'airline_flight_number', 'visa_type', 'gender'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-----------------+--------------+------------+------------+------------+------------------+-------------+------------+------------+---------------------+---------+------+\n",
      "|immigration_id|immigration_year|immigration_month|origin_country|arrival_port|arrival_date|   port_type|state_of_residence|immigrant_age|visitor_type|airline_code|airline_flight_number|visa_type|gender|\n",
      "+--------------+----------------+-----------------+--------------+------------+------------+------------+------------------+-------------+------------+------------+---------------------+---------+------+\n",
      "|           6.0|          2016.0|              4.0|       ECUADOR|         XXX|  2016-04-29|Not reported|              null|         37.0|    Pleasure|        null|                 null|       B2|  null|\n",
      "|           7.0|          2016.0|              4.0|    SOUTHKOREA|         ATL|  2016-04-07|         Air|                AL|         25.0|     Student|        null|                00296|       F1|     M|\n",
      "|          15.0|          2016.0|              4.0|       ALBANIA|         WAS|  2016-04-01|         Air|                MI|         55.0|    Pleasure|          OS|                   93|       B2|     M|\n",
      "|          16.0|          2016.0|              4.0|       ALBANIA|         NYC|  2016-04-01|         Air|                MA|         28.0|    Pleasure|          AA|                00199|       B2|  null|\n",
      "|          17.0|          2016.0|              4.0|       ALBANIA|         NYC|  2016-04-01|         Air|                MA|          4.0|    Pleasure|          AA|                00199|       B2|  null|\n",
      "+--------------+----------------+-----------------+--------------+------------+------------+------------+------------------+-------------+------------+------------+---------------------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_df_clean.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This is looking good already! For some columns it makes sense to have nulls but for some we might need to replace them. In the UDFs we already took care of some cases (i.e. unknown arrival ports are denoted as 'Other' etc.), let's see where we still have nulls: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-----------------+--------------+------------+------------+---------+------------------+-------------+------------+------------+---------------------+---------+------+\n",
      "|immigration_id|immigration_year|immigration_month|origin_country|arrival_port|arrival_date|port_type|state_of_residence|immigrant_age|visitor_type|airline_code|airline_flight_number|visa_type|gender|\n",
      "+--------------+----------------+-----------------+--------------+------------+------------+---------+------------------+-------------+------------+------------+---------------------+---------+------+\n",
      "|             0|               0|                0|             0|           0|           0|        0|            152592|          802|           0|       83627|                19549|        0|414269|\n",
      "+--------------+----------------+-----------------+--------------+------------+------------+---------+------------------+-------------+------------+------------+---------------------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_df_clean.select([count(when(isnull(c), c)).alias(c) for c in immigration_df_clean.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We make the following choices:\n",
    "- state_of_residence: we don't know, so we put Unknown\n",
    "- departure_date: likely the immigrant hasn't departed (i.e. not yet or permanent immigration), so we put NA\n",
    "- immigrant_age: we put -1 to make clear that we don't know\n",
    "- airline_code: this one can be either because it's not entered correctly (we have less missing flight numbers than airline codes so this happens) but can also be because the country is not entered via air, so putting Unknown to be safe\n",
    "- airfline_number: same as above\n",
    "- gender: unknown gender we put X generally denoted as unknown (or not willing to provide) gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_df_clean = (immigration_df_clean\n",
    "                            .fillna({'immigrant_age' : -1, 'state_of_residence' : 'unknown'\n",
    "                                     , 'airline_code' : 'Unkown'\n",
    "                                     , 'airline_flight_number' : 'Unknown', 'gender' : 'X'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_df_clean.createOrReplaceTempView('immigration_df_clean')\n",
    "immigration_df_clean = spark.sql('select row_number() over (order by \"immigration_id\") as immigrant_id, * from immigration_df_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+----------------+-----------------+--------------+------------+------------+------------+------------------+-------------+------------+------------+---------------------+---------+------+\n",
      "|immigrant_id|immigration_id|immigration_year|immigration_month|origin_country|arrival_port|arrival_date|   port_type|state_of_residence|immigrant_age|visitor_type|airline_code|airline_flight_number|visa_type|gender|\n",
      "+------------+--------------+----------------+-----------------+--------------+------------+------------+------------+------------------+-------------+------------+------------+---------------------+---------+------+\n",
      "|           1|           6.0|          2016.0|              4.0|       ECUADOR|         XXX|  2016-04-29|Not reported|           unknown|         37.0|    Pleasure|      Unkown|              Unknown|       B2|     X|\n",
      "|           2|     3308035.0|          2016.0|              4.0|        FRANCE|         NYC|  2016-04-18|         Air|                NY|         12.0|    Pleasure|          AF|                00022|       WT|     F|\n",
      "|           3|           7.0|          2016.0|              4.0|    SOUTHKOREA|         ATL|  2016-04-07|         Air|                AL|         25.0|     Student|      Unkown|                00296|       F1|     M|\n",
      "|           4|     3308036.0|          2016.0|              4.0|        FRANCE|         NYC|  2016-04-18|         Air|                NY|         12.0|    Pleasure|          AF|                00006|       WT|     F|\n",
      "|           5|          15.0|          2016.0|              4.0|       ALBANIA|         WAS|  2016-04-01|         Air|                MI|         55.0|    Pleasure|          OS|                   93|       B2|     M|\n",
      "+------------+--------------+----------------+-----------------+--------------+------------+------------+------------+------------------+-------------+------------+------------+---------------------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#final data\n",
    "immigration_df_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Demographics data\n",
    "\n",
    "The demographics data is fairly straightforward, we just read in the data and rename a few columns. There are a few duplicates which we drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographics_df_clean = (demographics_df.select('City', 'State Code',\n",
    "                                       'Male Population',\n",
    "                                       'Female Population',\n",
    "                                       'Total Population',\n",
    "                                       'Foreign-born',\n",
    "                                       'Race',\n",
    "                                        'Count') \n",
    "            .withColumnRenamed('City', 'city') \n",
    "            .withColumnRenamed('State Code', 'state') \n",
    "            .withColumnRenamed('Male Population', 'male_population') \n",
    "            .withColumnRenamed('Female Population', 'female_population') \n",
    "            .withColumnRenamed('Total Population', 'total_population') \n",
    "            .withColumnRenamed('Foreign-born', 'foreign_born')\n",
    "            .withColumnRenamed('Race', 'race')\n",
    "            .withColumnRenamed('Count', 'n_persons')\n",
    "            .withColumn('port', city_to_port('City')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+---------------+-----------------+----------------+------------+--------------------+---------+----+\n",
      "|            city|state|male_population|female_population|total_population|foreign_born|                race|n_persons|port|\n",
      "+----------------+-----+---------------+-----------------+----------------+------------+--------------------+---------+----+\n",
      "|   Silver Spring|   MD|          40601|            41862|           82463|       30908|  Hispanic or Latino|    25924|null|\n",
      "|          Quincy|   MA|          44129|            49500|           93629|       32935|               White|    58723|null|\n",
      "|          Hoover|   AL|          38040|            46799|           84839|        8229|               Asian|     4759|null|\n",
      "|Rancho Cucamonga|   CA|          88127|            87105|          175232|       33878|Black or African-...|    24437|null|\n",
      "|          Newark|   NJ|         138040|           143873|          281913|       86253|               White|    76402| NEW|\n",
      "+----------------+-----+---------------+-----------------+----------------+------------+--------------------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#final data\n",
    "demographics_df_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "For this project we choose a star model with dimension and fact tables. A star model makes sense in this case since we have a single unique object of study (the immigrant) and based on a few basic dimensions about the immigrant we can then extend those to more facts about the dimension. The dimension table is the immigrant table with an unique ID per immigrant. This table then contains a few fields that allow to join on fact tables that contain more information about that dimension. We propose the following:\n",
    "\n",
    "Dimension Table: immigrant\n",
    "- immigrant_id (to get to immigrant info, to get to arrival info)\n",
    "- arrival_port (to get to city info)\n",
    "- state_of_residence (to get to city info)\n",
    "- arrival_date (to get to arrival time date)\n",
    "\n",
    "Fact table: immigrant_stats\n",
    "- immigrant_id\n",
    "- immigration_id\n",
    "- gender\n",
    "- age\n",
    "- origin_country\n",
    "- visa_type\n",
    "- visitor_type\n",
    "\n",
    "Fact table: city_pop\n",
    "- city\n",
    "- port\n",
    "- state\n",
    "- male population\n",
    "- female population\n",
    "- total population\n",
    "- foreign born\n",
    "\n",
    "Fact table: city_demographics\n",
    "- city\n",
    "- port\n",
    "- state\n",
    "- race\n",
    "- n_persons\n",
    "\n",
    "Fact table: arrival_info\n",
    "- immigrant_id\n",
    "- arrival_port\n",
    "- port_type\n",
    "- airline_code\n",
    "- airline_flight_number\n",
    "\n",
    "Fact table: arrival_date\n",
    "- arrival_date\n",
    "- immigration_year\n",
    "- immigration_date\n",
    "\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1. First, we load the two cleaned spark tables for immigration and demographics data to S3\n",
    "2. Then we use Airflow to schedule staging tables to Redshift\n",
    "3. In the next step we create the star schema as outline above\n",
    "4. Finally, we add quality checks at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Step 1 below, please see ########### for steps 2-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "bucket = 's3a://jehofman-udacity-dend-capstone-project/'\n",
    "\n",
    "demographics_df_clean.write.json(\n",
    "            os.path.join(\n",
    "                bucket,\n",
    "                'demographics/'),\n",
    "            'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_df_clean.write.json(\n",
    "            os.path.join(\n",
    "                bucket,\n",
    "                'immigration/'),\n",
    "            'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
